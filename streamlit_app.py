# import asyncio

# Äáº£m báº£o ráº±ng cÃ³ vÃ²ng láº·p sá»± kiá»‡n (Event Loop) khi sá»­ dá»¥ng asyncio trong Streamlit
# try:
#     loop = asyncio.get_running_loop()
# except RuntimeError:
#     loop = asyncio.new_event_loop()
#     asyncio.set_event_loop(loop)
import os
import streamlit as st
from dotenv import load_dotenv
load_dotenv()
# === Giao diá»‡n Streamlit ===
st.set_page_config(
    page_title="Tra cá»©u Äiá»u lá»‡ ÄoÃ n",
    page_icon="https://quanlydoanvien.doanthanhnien.vn/favicon.ico",
    layout="wide"
)

# TrÃ¡nh lá»—i lazy loader cá»§a PyTorch khi dÃ¹ng vá»›i Streamlit
os.environ["PYTORCH_NO_LAZY_LOADER"] = "1"

from llama_index.core import Settings, PromptTemplate
from llama_index.core import VectorStoreIndex, SimpleDirectoryReader, StorageContext, load_index_from_storage

# Cáº¥u hÃ¬nh thÆ° má»¥c
DATA_DIR = "data"
PERSIST_DIR = "index_storage"
TTL = 24 * 60 * 60
# Replicate Credentials
with st.sidebar:
    st.title("ğŸ“˜ Tra cá»©u Äiá»u lá»‡ ÄoÃ n TNCS Há»“ ChÃ­ Minh")
    st.write('Chatbot há»— trá»£ bá»Ÿi Gemini.')
    if 'google' in st.secrets:
        st.success('KhÃ³a API Ä‘Ã£ Ä‘Æ°á»£c cung cáº¥p!', icon='âœ…')
        google_api_key = st.secrets["google"]["api_key"]
    else:
        google_api_key = st.text_input('Nháº­p Google API token:', type='password')
        if not (replicate_api.startswith('r8_') and len(replicate_api)==40):
            st.warning('Vui lÃ²ng nháº­p thÃ´ng tin Ä‘Äƒng nháº­p cá»§a báº¡n!', icon='âš ï¸')
        else:
            st.success('Tiáº¿n hÃ nh nháº­p tin nháº¯n nháº¯c nhá»Ÿ cá»§a báº¡n!', icon='ğŸ‘‰')
    os.environ['GOOGLE_API_KEY'] = google_api_key
    st.subheader('MÃ´ hÃ¬nh vÃ  thÃ´ng sá»‘')
    selected_model = st.sidebar.selectbox('Chá»n máº«u Gemini', ['Gemini 2.0 Flash', 'Gemini 2.0 Flash (Image Generation) Experimental'], key='selected_model')
    if selected_model == 'Gemini 2.0 Flash':
        model_name = 'gemini-2.0-flash'
    elif selected_model == 'Gemini 2.0 Flash (Image Generation) Experimental':
        model_name = 'gemini-2.0-flash-exp-image-generation'
# === Táº£i model tá»« HuggingFace Hub vá» local ===
def download_model_to_local(repo_id: str, local_dir: str = "models") -> None:
    from huggingface_hub import snapshot_download
    os.makedirs(local_dir, exist_ok=True)
    snapshot_download(repo_id=repo_id, local_dir=local_dir, local_dir_use_symlinks=False)

# === DÃ¹ng cache Ä‘á»ƒ trÃ¡nh reload model má»—i láº§n Streamlit refresh ===
@st.cache_data(ttl=TTL, show_spinner="Äang khá»Ÿi táº¡o GoogleGenAI")
def load_embed_model_gemini():
    from llama_index.llms.google_genai import GoogleGenAI
    return GoogleGenAI(model=model_name, api_key=google_api_key)

@st.cache_data(ttl=TTL, show_spinner="Äang khá»Ÿi táº¡o Embedding")
def load_embed_model():
    from llama_index.embeddings.huggingface import HuggingFaceEmbedding
    download_model_to_local(repo_id="sentence-transformers/all-MiniLM-L6-v2")
    return HuggingFaceEmbedding(model_name="sentence-transformers/all-MiniLM-L6-v2")
    # from llama_index.embeddings.google_genai import GoogleGenAIEmbedding
    # from google.genai.types import EmbedContentConfig
    # return GoogleGenAIEmbedding(
    #     model_name="text-embedding-004",
    #     api_key=google_api_key
    # )

embed_model = load_embed_model()
llm = load_embed_model_gemini()

Settings.llm = llm
Settings.embed_model = embed_model

# Khá»Ÿi táº¡o hoáº·c náº¡p chá»‰ má»¥c
@st.cache_data(ttl=TTL, show_spinner="Äang khá»Ÿi táº¡o dá»¯ liá»‡u")
def setup_index():
    if os.path.exists(os.path.join(PERSIST_DIR, "docstore.json")):
        print("ğŸ” Äang táº£i láº¡i chá»‰ má»¥c tá»« bá»™ nhá»›...")
        storage_context = StorageContext.from_defaults(persist_dir=PERSIST_DIR)
        return load_index_from_storage(storage_context)
    else:
        print("ğŸ› ï¸ Äang táº¡o má»›i chá»‰ má»¥c...")
        reader = SimpleDirectoryReader(input_dir=DATA_DIR)
        documents = reader.load_data()
        vsi = VectorStoreIndex.from_documents(documents)
        vsi.storage_context.persist(persist_dir=PERSIST_DIR)
        return vsi

# === Táº O PROMPT TÃ™Y CHá»ˆNH ===
QA_PROMPT_TMPL = (
    "Báº¡n lÃ  má»™t trá»£ lÃ½ hiá»ƒu biáº¿t vá» Äiá»u lá»‡ ÄoÃ n TNCS Há»“ ChÃ­ Minh. "
    "Tráº£ lá»i cÃ¡c cÃ¢u há»i dÆ°á»›i Ä‘Ã¢y má»™t cÃ¡ch chi tiáº¿t, chÃ­nh xÃ¡c vÃ  sá»­ dá»¥ng ngÃ´n ngá»¯ phÃ¡p lÃ½. "
    "Náº¿u cÃ³ thá»ƒ, hÃ£y trÃ­ch dáº«n cá»¥ thá»ƒ cÃ¡c Ä‘iá»u, khoáº£n trong Äiá»u lá»‡ ÄoÃ n. "
    "Náº¿u cÃ¢u há»i vÆ°á»£t quÃ¡ pháº¡m vi tÃ i liá»‡u, báº¡n cÃ³ thá»ƒ tÃ¬m kiáº¿m thÃªm tá»« cÃ¡c nguá»“n uy tÃ­n. "
    "\nCÃ¢u há»i: {query_str}\n"
    "TÃ i liá»‡u tham kháº£o:\n{context_str}\n\n"
    "Tráº£ lá»i:"
)
qa_prompt = PromptTemplate(QA_PROMPT_TMPL)

index = setup_index()
query_engine = index.as_query_engine(similarity_top_k=4)
query_engine.update_prompts({"response_synthesizer:text_qa_template": qa_prompt})

# Há»i Gemini tá»± do (fallback khi dá»¯ liá»‡u khÃ´ng Ä‘á»§)
def ask_gemini_directly(question):
    try:
        return llm.complete(f"{question} Trong trÆ°á»ng há»£p nÃ y, báº¡n cÃ³ thá»ƒ tÃ¬m kiáº¿m trÃªn web Ä‘á»ƒ cung cáº¥p thÃªm thÃ´ng tin.").text
    except Exception as e:
        return f"âŒ ÄÃ£ xáº£y ra lá»—i khi há»i Gemini: {e}"

# === Chat UI ===
st.header("ğŸ’¬ TrÃ² chuyá»‡n vá»›i AI Äiá»u lá»‡ ÄoÃ n")

# Store LLM generated responses
if "messages" not in st.session_state:
    st.session_state.messages = [
        {"role": "assistant", "content": "ChÃ o báº¡n! TÃ´i cÃ³ thá»ƒ há»— trá»£ gÃ¬ vá» Äiá»u lá»‡ ÄoÃ n TNCS Há»“ ChÃ­ Minh?"}
    ]

# Hiá»ƒn thá»‹ lá»‹ch sá»­ chat
for msg in st.session_state.messages:
    with st.chat_message(msg["role"]):
        st.markdown(msg["content"])

def clear_chat_history():
    st.session_state.messages = [{"role": "assistant", "content": "ChÃ o báº¡n! TÃ´i cÃ³ thá»ƒ há»— trá»£ gÃ¬ vá» Äiá»u lá»‡ ÄoÃ n TNCS Há»“ ChÃ­ Minh?"}]
st.sidebar.button('XÃ³a lá»‹ch sá»­ trÃ² chuyá»‡n', on_click=clear_chat_history)

# Nháº­n Ä‘áº§u vÃ o tá»« ngÆ°á»i dÃ¹ng
if prompt := st.chat_input("Nháº­p cÃ¢u há»i cá»§a báº¡n..."):
    st.session_state.messages.append({"role": "user", "content": prompt})
    with st.chat_message("user"):
        st.markdown(prompt)

    with st.chat_message("assistant"):
        with st.spinner("ğŸ” Äang tÃ¬m cÃ¢u tráº£ lá»i..."):
            response = query_engine.query(prompt)
            answer = response.response.strip()
            if len(answer) < 30:
                fallback = ask_gemini_directly(prompt)
                st.markdown("ğŸŒ **KhÃ´ng Ä‘á»§ dá»¯ liá»‡u ná»™i bá»™, Ä‘ang há»i Gemini vá»›i tÃ¬m kiáº¿m má»Ÿ rá»™ng...**")
                st.markdown(fallback)
                st.session_state.messages.append({"role": "assistant", "content": fallback})
            else:
                st.markdown("âœ… **Tráº£ lá»i tá»« Äiá»u lá»‡ ÄoÃ n:**")
                st.markdown(answer)
                st.session_state.messages.append({"role": "assistant", "content": answer})