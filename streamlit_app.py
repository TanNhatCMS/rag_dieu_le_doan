# import asyncio

# ƒê·∫£m b·∫£o r·∫±ng c√≥ v√≤ng l·∫∑p s·ª± ki·ªán (Event Loop) khi s·ª≠ d·ª•ng asyncio trong Streamlit
# try:
#     loop = asyncio.get_running_loop()
# except RuntimeError:
#     loop = asyncio.new_event_loop()
#     asyncio.set_event_loop(loop)
import os
import streamlit as st
from dotenv import load_dotenv
load_dotenv()
# === Giao di·ªán Streamlit ===
st.set_page_config(
    page_title="Tra c·ª©u ƒêi·ªÅu l·ªá ƒêo√†n",
    page_icon="https://quanlydoanvien.doanthanhnien.vn/favicon.ico",
    layout="wide"
)
st.title("üìò Tra c·ª©u ƒêi·ªÅu l·ªá ƒêo√†n TNCS H·ªì Ch√≠ Minh")

# Tr√°nh l·ªói lazy loader c·ªßa PyTorch khi d√πng v·ªõi Streamlit
os.environ["PYTORCH_NO_LAZY_LOADER"] = "1"

from llama_index.core import Settings, PromptTemplate
from llama_index.core import VectorStoreIndex, SimpleDirectoryReader, StorageContext, load_index_from_storage

# Load bi·∫øn m√¥i tr∆∞·ªùng
google_api_key = st.secrets["google"]["api_key"]
model_name = "gemini-2.0-flash"
os.environ["GOOGLE_API_KEY"]=google_api_key
# C·∫•u h√¨nh th∆∞ m·ª•c
DATA_DIR = "data"
PERSIST_DIR = "index_storage"
TTL = 24 * 60 * 60

# === T·∫£i model t·ª´ HuggingFace Hub v·ªÅ local ===
def download_model_to_local(repo_id: str, local_dir: str = "models") -> None:
    from huggingface_hub import snapshot_download
    os.makedirs(local_dir, exist_ok=True)
    snapshot_download(repo_id=repo_id, local_dir=local_dir, local_dir_use_symlinks=False)

# === D√πng cache ƒë·ªÉ tr√°nh reload model m·ªói l·∫ßn Streamlit refresh ===
@st.cache_data(ttl=TTL, show_spinner="ƒêang kh·ªüi t·∫°o GoogleGenAI")
def load_embed_model_gemini():
    from llama_index.llms.google_genai import GoogleGenAI
    return GoogleGenAI(model=model_name, api_key=google_api_key)

@st.cache_data(ttl=TTL, show_spinner="ƒêang kh·ªüi t·∫°o Embedding")
def load_embed_model():
    from llama_index.embeddings.huggingface import HuggingFaceEmbedding
    download_model_to_local(repo_id="sentence-transformers/all-MiniLM-L6-v2")
    return HuggingFaceEmbedding(model_name="sentence-transformers/all-MiniLM-L6-v2")
    # from llama_index.embeddings.google_genai import GoogleGenAIEmbedding
    # from google.genai.types import EmbedContentConfig
    # return GoogleGenAIEmbedding(
    #     model_name="text-embedding-004",
    #     api_key=google_api_key
    # )

embed_model = load_embed_model()
llm = load_embed_model_gemini()

Settings.llm = llm
Settings.embed_model = embed_model

# Kh·ªüi t·∫°o ho·∫∑c n·∫°p ch·ªâ m·ª•c
@st.cache_data(ttl=TTL, show_spinner="ƒêang kh·ªüi t·∫°o d·ªØ li·ªáu")
def setup_index():
    if os.path.exists(os.path.join(PERSIST_DIR, "docstore.json")):
        print("üîÅ ƒêang t·∫£i l·∫°i ch·ªâ m·ª•c t·ª´ b·ªô nh·ªõ...")
        storage_context = StorageContext.from_defaults(persist_dir=PERSIST_DIR)
        return load_index_from_storage(storage_context)
    else:
        print("üõ†Ô∏è ƒêang t·∫°o m·ªõi ch·ªâ m·ª•c...")
        reader = SimpleDirectoryReader(input_dir=DATA_DIR)
        documents = reader.load_data()
        vsi = VectorStoreIndex.from_documents(documents)
        vsi.storage_context.persist(persist_dir=PERSIST_DIR)
        return vsi

# === T·∫†O PROMPT T√ôY CH·ªàNH ===
QA_PROMPT_TMPL = (
    "B·∫°n l√† m·ªôt tr·ª£ l√Ω hi·ªÉu bi·∫øt v·ªÅ ƒêi·ªÅu l·ªá ƒêo√†n TNCS H·ªì Ch√≠ Minh. "
    "Tr·∫£ l·ªùi c√°c c√¢u h·ªèi d∆∞·ªõi ƒë√¢y m·ªôt c√°ch chi ti·∫øt, ch√≠nh x√°c v√† s·ª≠ d·ª•ng ng√¥n ng·ªØ ph√°p l√Ω. "
    "N·∫øu c√≥ th·ªÉ, h√£y tr√≠ch d·∫´n c·ª• th·ªÉ c√°c ƒëi·ªÅu, kho·∫£n trong ƒêi·ªÅu l·ªá ƒêo√†n. "
    "N·∫øu c√¢u h·ªèi v∆∞·ª£t qu√° ph·∫°m vi t√†i li·ªáu, b·∫°n c√≥ th·ªÉ t√¨m ki·∫øm th√™m t·ª´ c√°c ngu·ªìn uy t√≠n. "
    "\nC√¢u h·ªèi: {query_str}\n"
    "T√†i li·ªáu tham kh·∫£o:\n{context_str}\n\n"
    "Tr·∫£ l·ªùi:"
)
qa_prompt = PromptTemplate(QA_PROMPT_TMPL)

index = setup_index()
query_engine = index.as_query_engine(similarity_top_k=4)
query_engine.update_prompts({"response_synthesizer:text_qa_template": qa_prompt})

# H·ªèi Gemini t·ª± do (fallback khi d·ªØ li·ªáu kh√¥ng ƒë·ªß)
def ask_gemini_directly(question):
    try:
        return llm.complete(f"{question} Trong tr∆∞·ªùng h·ª£p n√†y, b·∫°n c√≥ th·ªÉ t√¨m ki·∫øm tr√™n web ƒë·ªÉ cung c·∫•p th√™m th√¥ng tin.").text
    except Exception as e:
        return f"‚ùå ƒê√£ x·∫£y ra l·ªói khi h·ªèi Gemini: {e}"

query = st.text_input("Nh·∫≠p c√¢u h·ªèi:", placeholder="V√≠ d·ª•: Quy·ªÅn c·ªßa ƒëo√†n vi√™n l√† g√¨?", key="query_input")
submit = st.button("üß† Tr·∫£ l·ªùi") or (query and st.session_state.query_input)
if submit:
    # check query non empty
    if not query:
        st.warning("‚ö†Ô∏è Vui l√≤ng nh·∫≠p c√¢u h·ªèi tr∆∞·ªõc khi nh·∫•n n√∫t.")
    else:
        with st.spinner("üîç ƒêang t√¨m c√¢u tr·∫£ l·ªùi..."):
            response = query_engine.query(query)
            answer = response.response.strip()

            if len(answer) < 30:
                st.markdown("üåê **Kh√¥ng ƒë·ªß d·ªØ li·ªáu n·ªôi b·ªô, ƒëang h·ªèi Gemini v·ªõi t√¨m ki·∫øm m·ªü r·ªông...**")
                fallback = ask_gemini_directly(query)
                st.markdown(fallback)
            else:
                st.markdown("‚úÖ **Tr·∫£ l·ªùi t·ª´ ƒêi·ªÅu l·ªá ƒêo√†n:**")
                st.markdown(answer)
